{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes for models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "class RegModel():\n",
    "    \"\"\"\n",
    "    Regression Model for Chapter 2.6\n",
    "    \"\"\"\n",
    "    def __init__(self, k, b):\n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return tf.matmul(X, self.k) + self.b\n",
    "\n",
    "    \n",
    "class LogRegModel(Model):\n",
    "    \"\"\"\n",
    "    Classification Model for Chapter 3.6 (simple logreg)\n",
    "    \"\"\"\n",
    "    def __init__(self, W, b):\n",
    "        super(LogRegModel, self).__init__()\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def call(self, X):\n",
    "        return tf.matmul(X, self.W) + self.b\n",
    "    \n",
    "    \n",
    "class HidLayerModel(Model):\n",
    "    \"\"\"\n",
    "    Classification Model for Chapter 3.6 (nn with hidden layer and dropout)\n",
    "    \"\"\"\n",
    "    def __init__(self, W_relu, b_relu, W_logit, b_logit, p_keep=0.5):\n",
    "        super(HidLayerModel, self).__init__()\n",
    "        self.W_relu = W_relu\n",
    "        self.b_relu = b_relu\n",
    "        self.W_logit = W_logit\n",
    "        self.b_logit = b_logit\n",
    "        self.p_keep = p_keep\n",
    "        \n",
    "    def call(self, X):\n",
    "        hidden_layer = tf.nn.relu(tf.matmul(X, self.W_relu) + self.b_relu)  \n",
    "        h_drop = tf.nn.dropout(hidden_layer, self.p_keep)\n",
    "        \n",
    "        return tf.matmul(h_drop, self.W_logit) + self.b_logit\n",
    "\n",
    "\n",
    "class BNModel1(Model):\n",
    "    \"\"\"\n",
    "    Model for Chapter 4.3 (with batch normalization)\n",
    "    \"\"\"\n",
    "    __name__ = 'BNModel1'\n",
    "    \n",
    "    def __init__(self, input_size=784, fc1_size=300, fc2_size=100, fc3_size=10):\n",
    "        super(BNModel1, self).__init__()\n",
    "        \n",
    "        self.W_1 = tf.Variable(tf.random.truncated_normal([input_size, fc1_size], stddev=0.1))\n",
    "        self.b_1 = tf.Variable(tf.random.truncated_normal([fc1_size], stddev=0.1))\n",
    "        \n",
    "        self.beta = tf.Variable(tf.zeros([fc1_size]))\n",
    "        self.scale = tf.Variable(tf.ones([fc1_size]))\n",
    "        \n",
    "        self.W_2 = tf.Variable(tf.random.truncated_normal([fc1_size, fc2_size], stddev=0.1))\n",
    "        self.b_2 = tf.Variable(tf.random.truncated_normal([fc2_size], stddev=0.1))       \n",
    "        \n",
    "        self.W_3 = tf.Variable(tf.random.truncated_normal([fc2_size, fc3_size], stddev=0.1))\n",
    "        self.b_3 = tf.Variable(tf.random.truncated_normal([fc3_size], stddev=0.1))  \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        # input layer\n",
    "        h1 = tf.nn.relu(tf.matmul(X, self.W_1) + self.b_1)\n",
    "        \n",
    "        # batch norm layer\n",
    "        batch_mean, batch_var = tf.nn.moments(h1, [0])\n",
    "        h1_bn = tf.nn.batch_normalization(h1, batch_mean, batch_var, self.beta, self.scale, 0.001)\n",
    "        \n",
    "        # hidden layer\n",
    "        h2 = tf.nn.relu(tf.matmul(h1_bn, self.W_2) + self.b_2)\n",
    "        \n",
    "        # output layer\n",
    "        return tf.nn.relu(tf.matmul(h2, self.W_3) + self.b_3)\n",
    "\n",
    "\n",
    "class BNModel2(Model):\n",
    "    \"\"\"\n",
    "    Model for Chapter 4.3 (without batch normalization)\n",
    "    \"\"\"\n",
    "    __name__ = 'BNModel2'\n",
    "    \n",
    "    def __init__(self, input_size=784, fc1_size=300, fc2_size=100, fc3_size=10):\n",
    "        super(BNModel2, self).__init__()\n",
    "        \n",
    "        self.W_1 = tf.Variable(tf.random.truncated_normal([input_size, fc1_size], stddev=0.1))\n",
    "        self.b_1 = tf.Variable(tf.random.truncated_normal([fc1_size], stddev=0.1))\n",
    "        \n",
    "        self.W_2 = tf.Variable(tf.random.truncated_normal([fc1_size, fc2_size], stddev=0.1))\n",
    "        self.b_2 = tf.Variable(tf.random.truncated_normal([fc2_size], stddev=0.1))       \n",
    "        \n",
    "        self.W_3 = tf.Variable(tf.random.truncated_normal([fc2_size, fc3_size], stddev=0.1))\n",
    "        self.b_3 = tf.Variable(tf.random.truncated_normal([fc3_size], stddev=0.1))  \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        # input layer\n",
    "        h1 = tf.nn.relu(tf.matmul(X, self.W_1) + self.b_1)\n",
    "                \n",
    "        # hidden layer\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, self.W_2) + self.b_2)\n",
    "        \n",
    "        # output layer\n",
    "        return tf.nn.relu(tf.matmul(h2, self.W_3) + self.b_3)\n",
    "    \n",
    "\n",
    "class SimpleModel(Model):\n",
    "    \"\"\"\n",
    "    Model for Chapter 4.5 (simple model)\n",
    "    \"\"\"\n",
    "    __name__ = 'SimpleModel'\n",
    "    \n",
    "    def __init__(self, input_size=784, fc1_size=300, fc2_size=100, fc3_size=10):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.W_1 = tf.Variable(tf.random.truncated_normal([input_size, fc1_size], stddev=0.1))\n",
    "        self.b_1 = tf.Variable(tf.random.truncated_normal([fc1_size], stddev=0.1))\n",
    "        \n",
    "        self.W_2 = tf.Variable(tf.random.truncated_normal([fc1_size, fc2_size], stddev=0.1))\n",
    "        self.b_2 = tf.Variable(tf.random.truncated_normal([fc2_size], stddev=0.1))       \n",
    "        \n",
    "        self.W_3 = tf.Variable(tf.random.truncated_normal([fc2_size, fc3_size], stddev=0.1))\n",
    "        self.b_3 = tf.Variable(tf.random.truncated_normal([fc3_size], stddev=0.1))  \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        # input layer\n",
    "        h1 = tf.nn.relu(tf.matmul(X, self.W_1) + self.b_1)\n",
    "\n",
    "        # hidden layer\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, self.W_2) + self.b_2)\n",
    "        \n",
    "        # output layer\n",
    "        return tf.nn.relu(tf.matmul(h2, self.W_3) + self.b_3)\n",
    "    \n",
    "\n",
    "class BNModel(Model):\n",
    "    \"\"\"\n",
    "    Model for Chapter 4.5 (model with Batch Normalization layer)\n",
    "    \"\"\"\n",
    "    __name__ = 'BNModel'\n",
    "    \n",
    "    def __init__(self, input_size=784, fc1_size=300, fc2_size=100, fc3_size=10):\n",
    "        super(BNModel, self).__init__()\n",
    "        \n",
    "        self.W_1 = tf.Variable(tf.random.truncated_normal([input_size, fc1_size], stddev=0.1))\n",
    "        self.b_1 = tf.Variable(tf.random.truncated_normal([fc1_size], stddev=0.1))\n",
    "        \n",
    "        self.beta = tf.Variable(tf.zeros([fc1_size]))\n",
    "        self.scale = tf.Variable(tf.ones([fc1_size]))\n",
    "        \n",
    "        self.W_2 = tf.Variable(tf.random.truncated_normal([fc1_size, fc2_size], stddev=0.1))\n",
    "        self.b_2 = tf.Variable(tf.random.truncated_normal([fc2_size], stddev=0.1))       \n",
    "        \n",
    "        self.W_3 = tf.Variable(tf.random.truncated_normal([fc2_size, fc3_size], stddev=0.1))\n",
    "        self.b_3 = tf.Variable(tf.random.truncated_normal([fc3_size], stddev=0.1))  \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        # input layer\n",
    "        h1 = tf.nn.relu(tf.matmul(X, self.W_1) + self.b_1)\n",
    "        \n",
    "        # batch norm layer\n",
    "        batch_mean, batch_var = tf.nn.moments(h1, [0])\n",
    "        h1_bn = tf.nn.batch_normalization(h1, batch_mean, batch_var, self.beta, self.scale, 0.001)\n",
    "        \n",
    "        # hidden layer\n",
    "        h2 = tf.nn.relu(tf.matmul(h1_bn, self.W_2) + self.b_2)\n",
    "        \n",
    "        # output layer\n",
    "        return tf.nn.relu(tf.matmul(h2, self.W_3) + self.b_3)\n",
    "    \n",
    "\n",
    "class XavierModel(Model):\n",
    "    \"\"\"\n",
    "    Model for Chapter 4.5 (model with Xavier init)\n",
    "    \"\"\"\n",
    "    __name__ = 'XavierModel'\n",
    "        \n",
    "    def __init__(self, input_size=784, fc1_size=300, fc2_size=100, fc3_size=10):\n",
    "        super(XavierModel, self).__init__()\n",
    "        \n",
    "        initializer = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.W_1 = tf.Variable(initializer(shape=(input_size, fc1_size)))\n",
    "        self.b_1 = tf.Variable(initializer(shape=(fc1_size,)))\n",
    "        \n",
    "        self.W_2 = tf.Variable(initializer(shape=(fc1_size, fc2_size)))\n",
    "        self.b_2 = tf.Variable(initializer(shape=(fc2_size,)))       \n",
    "        \n",
    "        self.W_3 = tf.Variable(initializer(shape=(fc2_size, fc3_size)))\n",
    "        self.b_3 = tf.Variable(initializer(shape=(fc3_size,)))  \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        # input layer\n",
    "        h1 = tf.nn.relu(tf.matmul(X, self.W_1) + self.b_1)\n",
    "        \n",
    "        # hidden layer\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, self.W_2) + self.b_2)\n",
    "        \n",
    "        # output layer\n",
    "        return tf.nn.relu(tf.matmul(h2, self.W_3) + self.b_3)\n",
    "    \n",
    "\n",
    "class XavierBNModel(Model):\n",
    "    \"\"\"\n",
    "    Model for Chapter 4.5 (model with Batch Normalization layer and Xavier initialization)\n",
    "    \"\"\"\n",
    "    __name__ = 'XavierBNModel'\n",
    "    \n",
    "    def __init__(self, input_size=784, fc1_size=300, fc2_size=100, fc3_size=10):\n",
    "        super(XavierBNModel, self).__init__()\n",
    "        \n",
    "        initializer = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.W_1 = tf.Variable(initializer(shape=(input_size, fc1_size)))\n",
    "        self.b_1 = tf.Variable(initializer(shape=(fc1_size,)))\n",
    "        \n",
    "        self.beta = tf.Variable(tf.zeros([fc1_size]))\n",
    "        self.scale = tf.Variable(tf.ones([fc1_size]))\n",
    "        \n",
    "        self.W_2 = tf.Variable(initializer(shape=(fc1_size, fc2_size)))\n",
    "        self.b_2 = tf.Variable(initializer(shape=(fc2_size,)))       \n",
    "        \n",
    "        self.W_3 = tf.Variable(initializer(shape=(fc2_size, fc3_size)))\n",
    "        self.b_3 = tf.Variable(initializer(shape=(fc3_size,)))  \n",
    "        \n",
    "    def __call__(self, X):\n",
    "        # input layer\n",
    "        h1 = tf.nn.relu(tf.matmul(X, self.W_1) + self.b_1)\n",
    "        \n",
    "        # batch norm layer\n",
    "        batch_mean, batch_var = tf.nn.moments(h1, [0])\n",
    "        h1_bn = tf.nn.batch_normalization(h1, batch_mean, batch_var, self.beta, self.scale, 0.001)\n",
    "        \n",
    "        # hidden layer\n",
    "        h2 = tf.nn.relu(tf.matmul(h1_bn, self.W_2) + self.b_2)\n",
    "        \n",
    "        # output layer\n",
    "        return tf.nn.relu(tf.matmul(h2, self.W_3) + self.b_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
